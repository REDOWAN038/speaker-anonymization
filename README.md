# Speaker Anonymization

**News: The main branch of the repository contains now the code to our latest paper, Probing the Feasibility of Multilingual Speaker Anonymization,
that has been accepted at Interspeech 2024. For the previous version, please go to the [prosody_cloning](https://github.com/DigitalPhonetics/speaker-anonymization/tree/prosody_cloning)  branch.**

This repository contains the speaker anonymization system developed at the Institute for Natural Language Processing 
(IMS) at the University of Stuttgart, Germany. The system is described in the following papers:

| Paper | Published at | Branch                                                                                                              | Demo |
|-------|--------------|---------------------------------------------------------------------------------------------------------------------|------|
| [Speaker Anonymization with Phonetic Intermediate Representations](https://www.isca-speech.org/archive/interspeech_2022/meyer22b_interspeech.html) | [Interspeech 2022](https://www.interspeech2022.org/) | [phonetic_representations](https://github.com/DigitalPhonetics/speaker-anonymization/tree/phonetic_representations) | [https://huggingface.co/spaces/sarinam/speaker-anonymization](https://huggingface.co/spaces/sarinam/speaker-anonymization) |
| [Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy](https://ieeexplore.ieee.org/document/10022601) | [SLT 2022](https://slt2022.org/) | [gan_embeddings](https://github.com/DigitalPhonetics/speaker-anonymization/tree/gan_embeddings)                     | [https://huggingface.co/spaces/sarinam/speaker-anonymization-gan](https://huggingface.co/spaces/sarinam/speaker-anonymization-gan) |
| [Prosody Is Not Identity: A Speaker Anonymization Approach Using Prosody Cloning](https://ieeexplore.ieee.org/document/10096607) | [ICASSP 2023](https://2023.ieeeicassp.org/) | [prosody_cloning](https://github.com/DigitalPhonetics/speaker-anonymization/tree/prosody_cloning)                   | - |
| Probing the Feasibility of Multilingual Speaker Anonymization | Soon at [Interspeech 2024](https://interspeech2024.org/) | [multilingual](https://github.com/DigitalPhonetics/speaker-anonymization/tree/multilingual)                         | coming soon |

If you want to see the code to the respective papers, go to the branch referenced in the table. The latest version 
of our system can be found here on the main branch.

**Check out our live demo on Hugging Face: [https://huggingface.co/spaces/sarinam/speaker-anonymization](https://huggingface.co/spaces/sarinam/speaker-anonymization)**

**Check also out [our contribution](https://www.voiceprivacychallenge.org/results-2022/docs/3___T04.pdf) to the [Voice Privacy Challenge 2022](https://www.voiceprivacychallenge.org/results-2022/)!**


## System Description
The system, as described in [our paper at ICASSP 2023](https://ieeexplore.ieee.org/document/10096607), consists of three steps:

(1) Three kinds of information are extracted from the input signal:
* the linguistic content using an E2E ASR, represented in form of phonetic transcriptions
* the prosody in form of phone-wise normalized pitch, energy and duration values
* the speaker embedding in form of GST style embeddings, trained together with the TTS

(2) Speaker-specific information are anonymized to hide the speaker identity
* this is done by sampling an artificial speaker embedding generated by a GAN (described [in this paper](https://ieeexplore.ieee.org/document/10022601))
and replacing the embedding of the input speaker with this artificial one. Using cosine distance, we make sure that the new embedding is not too similar to the 
original one
* we can further manipulate pitch and energy using random offsets (this is not done in the current configs)

(3) Synthesis of the anonymized speech
* using a FastSpeech2-like TTS and a HiFiGAN vocoder, as implemented in our speech synthesis toolkit [IMSToucan](https://github.com/DigitalPhonetics/IMS-Toucan), we use the different information paths to 
generate an anonymized version of the input speech

### Multilingual additions
Our current implementation is based on the structure in our Voice Privacy toolkit [VoicePAT](https://github.com/DigitalPhonetics/VoicePAT).

The main differences to the system described above are the replacements of ASR and TTS models by multilingual counterparts:
* We replace our monolingual custom ASR by OpenAI's [Whisper-large-v3](https://huggingface.co/openai/whisper-large-v3). This model supports 99 languages and includes its own language
identification (currently not used in our system). The output of the model are orthographic text transcriptions.
* We replace our monolingual TTS with a new version that supports 12 languages, described in the paper 
[Low-Resource Multilingual and Zero-Shot Multispeaker TTS](https://aclanthology.org/2022.aacl-main.56). All code and models are provided in 
[IMS Toucan v2.5](https://github.com/DigitalPhonetics/IMS-Toucan/tree/v2.5) which is included as a submodule in this repository

(We plan to soon update the TTS to our [latest version supporting over 7000 languages!](https://github.com/DigitalPhonetics/IMS-Toucan/releases/tag/v3.0))

![architecture](figures/architecture.png)


## Multilingual Data
We propose new speaker verification trials for multilingual speaker anonymization. For this, we use Multilingual LibriSpeech and CommonVoice.
You can find these new trials in the [trials_data](trials_data) folder.

**More information about these datasets will be added soon.**

## Audio Samples
**Will be added soon.**

## Installation
### 1. Clone repository
Clone this repository with all its submodules:
```
git clone --recurse-submodules https://github.com/DigitalPhonetics/speaker-anonymization.git
``` 

### 2. Download models
You will need to download the following models and specify the location to them in the respective config files:

For anonymization:

| Name | Function | Link | Location in config |
|------|----------|------|--------------------|
| embedding_function.pt | Speaker embedding encoder | [https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/embedding_function.pt](https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/embedding_function.pt) | modules > speaker_embeddings > embed_model_path && modules > tts > embeddings_path |
| embedding_gan.pt | Artificial speaker embeddings generator | [https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/embedding_gan.pt](https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/embedding_gan.pt) | tbd |
| aligner.pt | Prosody aligner | [https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/aligner.pt](https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/aligner.pt) | modules > prosody > aligner_model_path |
| ToucanTTS_Meta.pt | TTS model | [https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/ToucanTTS_Meta.pt](https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/ToucanTTS_Meta.pt) | modules > tts > fastspeech_path |
| Avocodo.pt | Vocoder | [https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/Avocodo.pt](https://github.com/DigitalPhonetics/IMS-Toucan/releases/download/v2.5/Avocodo.pt) | modules > tts > hifigan_path |
| asr_branchformer_tts-phn_en.zip | Monolingual ASR model | [https://github.com/DigitalPhonetics/speaker-anonymization/releases/download/v2.0/asr_branchformer_tts-phn_en.zip](https://github.com/DigitalPhonetics/speaker-anonymization/releases/download/v2.0/asr_branchformer_tts-phn_en.zip) | modules > asr > model_path |

For evaluation:

| Name | Function             | Link | Location in config |
|------|----------------------|------|--------------------|
| asv_pre_ecapa | Pretrained ASV model | [https://github.com/DigitalPhonetics/VoicePAT/releases/download/v1/pre_eval_models.zip](https://github.com/DigitalPhonetics/VoicePAT/releases/download/v1/pre_eval_models.zip) | privacy > asv > model_dir |
You need to unzip the pre_eval_models.zip first. The folders contain some more models that you don't need for the experiments given in the configs.

The whisper model is downloaded automatically.

### 4. Install requirements
Create a virtual environment and install the [requirements](requirements.txt). The current code has been tested with Python 3.10.
```
pip install -r requirements.txt
```

### 4. Prepare data
As described above, we include the trial data for Multilingual LibriSpeech and CommonVoice in several languages.
Before you can use them, you need to prepare the data information in kaldi format. For this, simply run the following command:
```
python run_prepare_data.py --mls_path <path-to-MLS-corpus> --cv_path <path-to-CV-corpus> --output_path <path-to-output-files>
```
You need to specify the location of the [MLS](http://www.openslr.org/94) and [CommonVoice](https://commonvoice.mozilla.org/en/datasets) corpora. 
If you don't already have them on your computer, you need to download these corpora first.
Note that we expect that you have CommonVoice version 16.1 or higher for all languages.

`<path-to-MLS-corpus>` should point to the root directory of the MLS dataset, in which folders like `mls_dutch` are located.
`<path-to-CV-corpus>` should point to the root directory for the CommonVoice version, in which folders like `nl` are located.
`<path-to-output-files>` points to `data` as subfolder of this repository by default.

If you want to test the model on the standard voice privacy evaluation splits for English, and train the ASV model on LibriSpeech train-clean-360,
please go to the [VPC 2022 website](https://github.com/Voice-Privacy-Challenge/Voice-Privacy-Challenge-2022) to request data access. These files should also be located in your `<path-to-output-files>`.


## Running the anonymization and evaluation pipelines
All settings in the pipeline are controlled in config files, located in the [configs](configs) folder. 
Before running any scripts, make sure that you set all paths in these configs correctly.
Anonymization and evaluation are executed in separate pipelines. You can run them with simple commands:

### Anonymization:
```
python run_anonymization.py --config anon/anon_ims_sttts_pc_whisper.yaml --lang <lang> --gpu_ids <gpu_ids>
```
`<lang>` is the tag of the language you want to run the anonymization for, e.g., `en`, `de`, `it`.
`<gpu_ids>` is a string of one or several GPU IDs you want the anonymization to use, e.g., `0` or `0,2,4`.

### Evaluation:
There are two types of evaluation configs. If you are confused about this, please check out the [evaluation plan of the VPC 2022](https://arxiv.org/abs/2203.12468).

#### Evaluation with models trained on original data (eval_pre)
```
python run_evaluation.py --config eval_pre/eval_pre_whisper.yaml --lang <lang> --gpu_ids <gpu_ids>
```

#### ASV evaluation with a model trained on anonymized data (eval_post)
For this, you first need to run the pipeline for English to train the model. Make sure that you have anonymized the libri-clean-360 first (we need this as training data). 

*Note: In the current version, we finetune the pretrained model on only a part of the anonymized libri-clean-360. You can find the kaldi files for this part in the [data.zip](https://github.com/DigitalPhonetics/VoicePAT/releases/download/v2/data.zip) of VoicePAT.*
```
python run_evaluation.py --config eval_post/eval_post_asv_en_training.yaml --lang en --gpu_ids <gpu_ids>
```

After that, you can use this model for other languages too:
```
python run_evaluation.py --config eval_post/eval_post_asv_with_trained_model.yaml --lang <lang> --gpu_ids <gpu_ids>
```

## Citations
```
@inproceedings{meyer2022speaker,
  author={Sarina Meyer and Florian Lux and Pavel Denisov and Julia Koch and Pascal Tilli and Ngoc Thang Vu},
  title={{Speaker Anonymization with Phonetic Intermediate Representations}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={4925--4929},
  doi={10.21437/Interspeech.2022-10703}
}
@inproceedings{meyer2023anonymizing,
  author={Meyer, Sarina and Tilli, Pascal and Denisov, Pavel and Lux, Florian and Koch, Julia and Vu, Ngoc Thang},
  booktitle={2022 IEEE Spoken Language Technology Workshop (SLT)}, 
  title={Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy}, 
  year={2023},
  pages={912-919},
  doi={10.1109/SLT54892.2023.10022601}
 }
@inproceedings{meyer2023prosody,
  author={Meyer, Sarina and Lux, Florian and Koch, Julia and Denisov, Pavel and Tilli, Pascal and Vu, Ngoc Thang},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Prosody Is Not Identity: A Speaker Anonymization Approach Using Prosody Cloning}, 
  year={2023},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10096607}
}
```

